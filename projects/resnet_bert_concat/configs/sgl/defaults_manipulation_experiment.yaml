inference:
  model: resnet_bert_concat
  checkpoint_path: ../me_sgl/pretrained_model/sgl_model.ckpt
  root: ../me_sgl/data/
  save_root: ../me_sgl/result/
  task_type: cut_task
  difficulty_level: easy

env:
  data_dir: ${env.cache_dir}/data

model_config:
  resnet_bert_concat:
    model: resnet_bert_concat
    # Type of bert model
    bert_model_name: bert-base-uncased
    direct_features_input: false
    # Dimension of the embedding finally returned by the modal encoder
    modal_hidden_size: 512 
    # Dimension of the embedding finally returned by the text encoder
    text_hidden_size: 768 
    # Used when classification head is activated
    num_labels: 2
    # Number of features extracted out per image
    num_features: 1

    image_encoder:
      type: resnet34 #resnet34, resnet152
      params:
        pretrained: true
        pool_type: avg
        num_output_features: 1 
        pretrained_model: false
    text_encoder:
      type: transformer
      params:
        bert_model_name: bert-base-uncased
        hidden_size: 768
        num_hidden_layers: 12
        num_attention_heads: 12
        output_attentions: false
        output_hidden_states: false
        pretrained_model: false

    classifier_state:
      type: mlp
      params:
        in_dim: 1280
        out_dim: 5
        hidden_dim: 256
        num_layers: 2
        
    classifier_subject:
      type: mlp
      params:
        in_dim: 1280
        out_dim: 35
        hidden_dim: 256 
        num_layers: 2

    classifier_object:
      type: mlp
      params:
        in_dim: 1280 
        out_dim: 35 
        hidden_dim: 256 
        num_layers: 2

dataset_config:
  sgl:
      data_dir: ${env.data_dir}/datasets
      depth_first: false
      fast_read: false
      use_images: true
      use_features: false
      zoo_requirements:
      - sgl.defaults
      images:
        train:
        - sgl/defaults/images/train
        val:
        - sgl/defaults/images/val
        test:
        - sgl/defaults/images/test
      annotations:
        train:
        - sgl/defaults/annotations/imdb_train.npy
        val:
        - sgl/defaults/annotations/imdb_val.npy
        test:
        - sgl/defaults/annotations/imdb_test.npy
      max_features: 100
      processors:
        image_processor:
          type: torchvision_transforms
          params:
            transforms:
            - type: Resize
              params:
                  size: [512, 512]
            - type: CenterCrop
              params:
                size: [512, 512]
            - ToTensor
            - GrayScaleTo3Channels
            - type: Normalize
              params:
                mean: [0.249, 0.249, 0.249]
                std: [0.432, 0.433, 0.432]
        text_processor:
          type: bert_tokenizer
          params:
            tokenizer_config:
              type: bert-base-uncased
              params:
                do_lower_case: true
            mask_probability: 0
            max_seq_length: 30
        answer_processor:
          type: sgl_result
          params:
            vocab_state_file: ${dataset_config.sgl.data_dir}/sgl/defaults/extras/vocabs/state_gtrtr.txt
            vocab_object_file: ${dataset_config.sgl.data_dir}/sgl/defaults/extras/vocabs/object_gtrtr.txt
            preprocessor:
              type: simple_word
              params: {}
        context_processor:
          type: fasttext
          params:
            download_initially: false
            max_length: 50
            model_file: wiki.en.bin

      return_features_info: false
      # Return OCR information
      use_ocr: false
      # Return spatial information of OCR tokens if present
      use_ocr_info: false

optimizer:
  type: adam_w_skip_params_with_zero_grad
  params:
    lr: 5e-05
    weight_decay: 0
    eps: 1e-09  
    betas:
    - 0.9
    - 0.98

scheduler:
  type: warmup_cosine
  params:
    num_warmup_steps: 6444
    num_training_steps: 128880

evaluation:
  metrics:
  - sgl_accuracy
  visualize: false

training:
  lr_scheduler: true
  clip_norm_mode: all
  clip_gradients: false
  max_grad_l2_norm: 5
  max_updates: 128880
  batch_size: 4
  task_size_proportional_sampling: true
  encoder_lr_multiply: 1
  early_stop:
    criteria: sgl/sgl_accuracy
    minimize: false

  find_unused_parameters: true
